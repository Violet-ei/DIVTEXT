INFO -> 2025-12-06 22:22:01,615: sst2, args: Namespace(log_path='./log', dataset='sst2', save_path='./trained_model', model_type='bert-base-uncased', epochs=3, num_labels=2, lr=2e-05, max_len=128, batch_size=64, use_cuda=True, num_workers=32, seed=42, log_steps=50, eval_steps=50, eps=3.0, top_k=20, embedding_type='ct_vectors', mapping_strategy='conservative', privatization_strategy='s1', save_stop_words=False)
INFO -> 2025-12-06 22:22:39,116: train_data:67349,dev_data:872,test_data:1821
INFO -> 2025-12-06 22:22:48,173: Init_val_acc: 0.433828
INFO -> 2025-12-06 22:22:48,173: Training model for 3 epochs..
INFO -> 2025-12-06 22:22:48,461: [Train] epoch:1/3, step: 0/3159, step_loss:0.6868
INFO -> 2025-12-06 22:23:30,448: [Evaluate] epoch:1/3, step: 50/3159, val_acc:0.46623
INFO -> 2025-12-06 22:23:30,875: [Evaluate] best accuracy performance has been updated: 0.43383 -> 0.46623
INFO -> 2025-12-06 22:23:31,155: [Train] epoch:1/3, step: 50/3159, step_loss:0.5395
INFO -> 2025-12-06 22:24:14,063: [Evaluate] epoch:1/3, step: 100/3159, val_acc:0.46074
INFO -> 2025-12-06 22:24:14,331: [Train] epoch:1/3, step: 100/3159, step_loss:0.5211
INFO -> 2025-12-06 22:24:57,489: [Evaluate] epoch:1/3, step: 150/3159, val_acc:0.48435
INFO -> 2025-12-06 22:24:57,857: [Evaluate] best accuracy performance has been updated: 0.46623 -> 0.48435
INFO -> 2025-12-06 22:24:58,122: [Train] epoch:1/3, step: 150/3159, step_loss:0.5115
INFO -> 2025-12-06 22:25:41,433: [Evaluate] epoch:1/3, step: 200/3159, val_acc:0.44481
INFO -> 2025-12-06 22:25:41,707: [Train] epoch:1/3, step: 200/3159, step_loss:0.4294
INFO -> 2025-12-06 22:26:25,189: [Evaluate] epoch:1/3, step: 250/3159, val_acc:0.57935
INFO -> 2025-12-06 22:26:25,573: [Evaluate] best accuracy performance has been updated: 0.48435 -> 0.57935
INFO -> 2025-12-06 22:26:25,832: [Train] epoch:1/3, step: 250/3159, step_loss:0.4846
INFO -> 2025-12-06 22:27:09,388: [Evaluate] epoch:1/3, step: 300/3159, val_acc:0.56892
INFO -> 2025-12-06 22:27:09,654: [Train] epoch:1/3, step: 300/3159, step_loss:0.4971
INFO -> 2025-12-06 22:27:53,408: [Evaluate] epoch:1/3, step: 350/3159, val_acc:0.40912
INFO -> 2025-12-06 22:27:53,695: [Train] epoch:1/3, step: 350/3159, step_loss:0.3747
INFO -> 2025-12-06 22:28:37,638: [Evaluate] epoch:1/3, step: 400/3159, val_acc:0.52224
INFO -> 2025-12-06 22:28:37,922: [Train] epoch:1/3, step: 400/3159, step_loss:0.4016
INFO -> 2025-12-06 22:29:21,770: [Evaluate] epoch:1/3, step: 450/3159, val_acc:0.47556
INFO -> 2025-12-06 22:29:22,034: [Train] epoch:1/3, step: 450/3159, step_loss:0.4385
INFO -> 2025-12-06 22:30:05,935: [Evaluate] epoch:1/3, step: 500/3159, val_acc:0.41351
INFO -> 2025-12-06 22:30:06,205: [Train] epoch:1/3, step: 500/3159, step_loss:0.3557
INFO -> 2025-12-06 22:30:50,477: [Evaluate] epoch:1/3, step: 550/3159, val_acc:0.47501
INFO -> 2025-12-06 22:30:50,753: [Train] epoch:1/3, step: 550/3159, step_loss:0.4187
INFO -> 2025-12-06 22:31:34,927: [Evaluate] epoch:1/3, step: 600/3159, val_acc:0.50192
INFO -> 2025-12-06 22:31:35,209: [Train] epoch:1/3, step: 600/3159, step_loss:0.4347
INFO -> 2025-12-06 22:32:19,322: [Evaluate] epoch:1/3, step: 650/3159, val_acc:0.47996
INFO -> 2025-12-06 22:32:19,600: [Train] epoch:1/3, step: 650/3159, step_loss:0.4504
INFO -> 2025-12-06 22:33:03,934: [Evaluate] epoch:1/3, step: 700/3159, val_acc:0.47941
INFO -> 2025-12-06 22:33:04,235: [Train] epoch:1/3, step: 700/3159, step_loss:0.3759
INFO -> 2025-12-06 22:33:48,505: [Evaluate] epoch:1/3, step: 750/3159, val_acc:0.44865
INFO -> 2025-12-06 22:33:48,783: [Train] epoch:1/3, step: 750/3159, step_loss:0.3745
INFO -> 2025-12-06 22:34:33,129: [Evaluate] epoch:1/3, step: 800/3159, val_acc:0.41406
INFO -> 2025-12-06 22:34:33,419: [Train] epoch:1/3, step: 800/3159, step_loss:0.3794
INFO -> 2025-12-06 22:35:17,723: [Evaluate] epoch:1/3, step: 850/3159, val_acc:0.42065
INFO -> 2025-12-06 22:35:18,002: [Train] epoch:1/3, step: 850/3159, step_loss:0.3924
INFO -> 2025-12-06 22:36:02,203: [Evaluate] epoch:1/3, step: 900/3159, val_acc:0.51071
INFO -> 2025-12-06 22:36:02,479: [Train] epoch:1/3, step: 900/3159, step_loss:0.6326
INFO -> 2025-12-06 22:36:46,687: [Evaluate] epoch:1/3, step: 950/3159, val_acc:0.45030
INFO -> 2025-12-06 22:36:46,961: [Train] epoch:1/3, step: 950/3159, step_loss:0.3486
INFO -> 2025-12-06 22:37:31,156: [Evaluate] epoch:1/3, step: 1000/3159, val_acc:0.44756
INFO -> 2025-12-06 22:37:31,429: [Train] epoch:1/3, step: 1000/3159, step_loss:0.4392
INFO -> 2025-12-06 22:38:15,631: [Evaluate] epoch:1/3, step: 1050/3159, val_acc:0.41021
INFO -> 2025-12-06 22:38:15,899: [Train] epoch:1/3, step: 1050/3159, step_loss:0.2941
INFO -> 2025-12-06 22:38:25,363: [Epoch 1] train_epoch_loss = 0.0073,  ---- val_acc = 0.3822,  [929.0s]
INFO -> 2025-12-06 22:39:07,076: [Evaluate] epoch:2/3, step: 1100/3159, val_acc:0.41406
INFO -> 2025-12-06 22:39:07,353: [Train] epoch:2/3, step: 1100/3159, step_loss:0.2938
INFO -> 2025-12-06 22:39:51,668: [Evaluate] epoch:2/3, step: 1150/3159, val_acc:0.47172
INFO -> 2025-12-06 22:39:51,943: [Train] epoch:2/3, step: 1150/3159, step_loss:0.4002
INFO -> 2025-12-06 22:40:36,535: [Evaluate] epoch:2/3, step: 1200/3159, val_acc:0.45470
INFO -> 2025-12-06 22:40:36,821: [Train] epoch:2/3, step: 1200/3159, step_loss:0.3827
INFO -> 2025-12-06 22:41:21,241: [Evaluate] epoch:2/3, step: 1250/3159, val_acc:0.40582
INFO -> 2025-12-06 22:41:21,524: [Train] epoch:2/3, step: 1250/3159, step_loss:0.3594
INFO -> 2025-12-06 22:42:06,138: [Evaluate] epoch:2/3, step: 1300/3159, val_acc:0.48380
INFO -> 2025-12-06 22:42:06,416: [Train] epoch:2/3, step: 1300/3159, step_loss:0.3515
INFO -> 2025-12-06 22:42:51,064: [Evaluate] epoch:2/3, step: 1350/3159, val_acc:0.47666
INFO -> 2025-12-06 22:42:51,346: [Train] epoch:2/3, step: 1350/3159, step_loss:0.3369
INFO -> 2025-12-06 22:43:35,868: [Evaluate] epoch:2/3, step: 1400/3159, val_acc:0.56398
INFO -> 2025-12-06 22:43:36,151: [Train] epoch:2/3, step: 1400/3159, step_loss:0.3648
INFO -> 2025-12-06 22:44:20,596: [Evaluate] epoch:2/3, step: 1450/3159, val_acc:0.44536
INFO -> 2025-12-06 22:44:20,867: [Train] epoch:2/3, step: 1450/3159, step_loss:0.4456
INFO -> 2025-12-06 22:45:05,331: [Evaluate] epoch:2/3, step: 1500/3159, val_acc:0.43053
INFO -> 2025-12-06 22:45:05,603: [Train] epoch:2/3, step: 1500/3159, step_loss:0.5711
INFO -> 2025-12-06 22:45:50,214: [Evaluate] epoch:2/3, step: 1550/3159, val_acc:0.51400
INFO -> 2025-12-06 22:45:50,494: [Train] epoch:2/3, step: 1550/3159, step_loss:0.2524
INFO -> 2025-12-06 22:46:34,940: [Evaluate] epoch:2/3, step: 1600/3159, val_acc:0.48325
INFO -> 2025-12-06 22:46:35,219: [Train] epoch:2/3, step: 1600/3159, step_loss:0.3002
INFO -> 2025-12-06 22:47:19,559: [Evaluate] epoch:2/3, step: 1650/3159, val_acc:0.48984
INFO -> 2025-12-06 22:47:19,833: [Train] epoch:2/3, step: 1650/3159, step_loss:0.3846
INFO -> 2025-12-06 22:48:04,289: [Evaluate] epoch:2/3, step: 1700/3159, val_acc:0.55244
INFO -> 2025-12-06 22:48:04,570: [Train] epoch:2/3, step: 1700/3159, step_loss:0.3664
INFO -> 2025-12-06 22:48:49,116: [Evaluate] epoch:2/3, step: 1750/3159, val_acc:0.50851
INFO -> 2025-12-06 22:48:49,388: [Train] epoch:2/3, step: 1750/3159, step_loss:0.3521
INFO -> 2025-12-06 22:49:34,044: [Evaluate] epoch:2/3, step: 1800/3159, val_acc:0.51290
INFO -> 2025-12-06 22:49:34,318: [Train] epoch:2/3, step: 1800/3159, step_loss:0.2313
INFO -> 2025-12-06 22:50:18,751: [Evaluate] epoch:2/3, step: 1850/3159, val_acc:0.50192
INFO -> 2025-12-06 22:50:19,031: [Train] epoch:2/3, step: 1850/3159, step_loss:0.2274
INFO -> 2025-12-06 22:51:03,414: [Evaluate] epoch:2/3, step: 1900/3159, val_acc:0.48435
INFO -> 2025-12-06 22:51:03,689: [Train] epoch:2/3, step: 1900/3159, step_loss:0.2426
INFO -> 2025-12-06 22:51:48,167: [Evaluate] epoch:2/3, step: 1950/3159, val_acc:0.48105
INFO -> 2025-12-06 22:51:48,443: [Train] epoch:2/3, step: 1950/3159, step_loss:0.3732
INFO -> 2025-12-06 22:52:33,065: [Evaluate] epoch:2/3, step: 2000/3159, val_acc:0.44646
INFO -> 2025-12-06 22:52:33,341: [Train] epoch:2/3, step: 2000/3159, step_loss:0.3418
INFO -> 2025-12-06 22:53:18,121: [Evaluate] epoch:2/3, step: 2050/3159, val_acc:0.38495
INFO -> 2025-12-06 22:53:18,407: [Train] epoch:2/3, step: 2050/3159, step_loss:0.2885
INFO -> 2025-12-06 22:54:02,845: [Evaluate] epoch:2/3, step: 2100/3159, val_acc:0.49259
INFO -> 2025-12-06 22:54:03,124: [Train] epoch:2/3, step: 2100/3159, step_loss:0.4099
INFO -> 2025-12-06 22:54:14,572: [Epoch 2] train_epoch_loss = 0.0051,  ---- val_acc = 0.4865,  [941.3s]
INFO -> 2025-12-06 22:54:54,220: [Evaluate] epoch:3/3, step: 2150/3159, val_acc:0.50192
INFO -> 2025-12-06 22:54:54,498: [Train] epoch:3/3, step: 2150/3159, step_loss:0.3731
INFO -> 2025-12-06 22:55:39,187: [Evaluate] epoch:3/3, step: 2200/3159, val_acc:0.47172
INFO -> 2025-12-06 22:55:39,464: [Train] epoch:3/3, step: 2200/3159, step_loss:0.2645
INFO -> 2025-12-06 22:56:24,235: [Evaluate] epoch:3/3, step: 2250/3159, val_acc:0.44426
INFO -> 2025-12-06 22:56:24,510: [Train] epoch:3/3, step: 2250/3159, step_loss:0.3528
INFO -> 2025-12-06 22:57:08,975: [Evaluate] epoch:3/3, step: 2300/3159, val_acc:0.45085
INFO -> 2025-12-06 22:57:09,256: [Train] epoch:3/3, step: 2300/3159, step_loss:0.3603
INFO -> 2025-12-06 22:57:54,030: [Evaluate] epoch:3/3, step: 2350/3159, val_acc:0.49643
INFO -> 2025-12-06 22:57:54,300: [Train] epoch:3/3, step: 2350/3159, step_loss:0.1477
INFO -> 2025-12-06 22:58:38,944: [Evaluate] epoch:3/3, step: 2400/3159, val_acc:0.49204
INFO -> 2025-12-06 22:58:39,228: [Train] epoch:3/3, step: 2400/3159, step_loss:0.2185
INFO -> 2025-12-06 22:59:23,891: [Evaluate] epoch:3/3, step: 2450/3159, val_acc:0.48874
INFO -> 2025-12-06 22:59:24,163: [Train] epoch:3/3, step: 2450/3159, step_loss:0.1815
INFO -> 2025-12-06 23:00:08,650: [Evaluate] epoch:3/3, step: 2500/3159, val_acc:0.47831
INFO -> 2025-12-06 23:00:08,925: [Train] epoch:3/3, step: 2500/3159, step_loss:0.2629
INFO -> 2025-12-06 23:00:53,392: [Evaluate] epoch:3/3, step: 2550/3159, val_acc:0.44591
INFO -> 2025-12-06 23:00:53,672: [Train] epoch:3/3, step: 2550/3159, step_loss:0.1893
INFO -> 2025-12-06 23:01:38,349: [Evaluate] epoch:3/3, step: 2600/3159, val_acc:0.46897
INFO -> 2025-12-06 23:01:38,623: [Train] epoch:3/3, step: 2600/3159, step_loss:0.1288
INFO -> 2025-12-06 23:02:23,160: [Evaluate] epoch:3/3, step: 2650/3159, val_acc:0.44152
INFO -> 2025-12-06 23:02:23,434: [Train] epoch:3/3, step: 2650/3159, step_loss:0.1626
INFO -> 2025-12-06 23:03:08,078: [Evaluate] epoch:3/3, step: 2700/3159, val_acc:0.47556
INFO -> 2025-12-06 23:03:08,355: [Train] epoch:3/3, step: 2700/3159, step_loss:0.1377
INFO -> 2025-12-06 23:03:53,038: [Evaluate] epoch:3/3, step: 2750/3159, val_acc:0.47227
INFO -> 2025-12-06 23:03:53,317: [Train] epoch:3/3, step: 2750/3159, step_loss:0.3285
INFO -> 2025-12-06 23:04:37,810: [Evaluate] epoch:3/3, step: 2800/3159, val_acc:0.50137
INFO -> 2025-12-06 23:04:38,090: [Train] epoch:3/3, step: 2800/3159, step_loss:0.3436
INFO -> 2025-12-06 23:05:22,599: [Evaluate] epoch:3/3, step: 2850/3159, val_acc:0.46183
INFO -> 2025-12-06 23:05:22,871: [Train] epoch:3/3, step: 2850/3159, step_loss:0.2054
INFO -> 2025-12-06 23:06:07,310: [Evaluate] epoch:3/3, step: 2900/3159, val_acc:0.47007
INFO -> 2025-12-06 23:06:07,587: [Train] epoch:3/3, step: 2900/3159, step_loss:0.2854
INFO -> 2025-12-06 23:06:52,192: [Evaluate] epoch:3/3, step: 2950/3159, val_acc:0.45909
INFO -> 2025-12-06 23:06:52,473: [Train] epoch:3/3, step: 2950/3159, step_loss:0.2444
INFO -> 2025-12-06 23:07:36,929: [Evaluate] epoch:3/3, step: 3000/3159, val_acc:0.45689
INFO -> 2025-12-06 23:07:37,208: [Train] epoch:3/3, step: 3000/3159, step_loss:0.2185
INFO -> 2025-12-06 23:08:21,979: [Evaluate] epoch:3/3, step: 3050/3159, val_acc:0.47941
INFO -> 2025-12-06 23:08:22,256: [Train] epoch:3/3, step: 3050/3159, step_loss:0.2040
INFO -> 2025-12-06 23:09:06,878: [Evaluate] epoch:3/3, step: 3100/3159, val_acc:0.45964
INFO -> 2025-12-06 23:09:07,167: [Train] epoch:3/3, step: 3100/3159, step_loss:0.1631
INFO -> 2025-12-06 23:09:51,632: [Evaluate] epoch:3/3, step: 3150/3159, val_acc:0.45799
INFO -> 2025-12-06 23:09:51,900: [Train] epoch:3/3, step: 3150/3159, step_loss:0.1904
INFO -> 2025-12-06 23:10:05,355: [Evaluate] epoch:3/3, step: 3158/3159, val_acc:0.45799
INFO -> 2025-12-06 23:10:13,421: [Epoch 3] train_epoch_loss = 0.0037,  ---- val_acc = 0.4580,  [950.9s]
INFO -> 2025-12-06 23:10:13,421: -- Training done in 2845s.
INFO -> 2025-12-06 23:10:21,513: test acc = 0.5794.
